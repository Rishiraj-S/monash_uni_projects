{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uZUNG-cXPwfq"
   },
   "outputs": [],
   "source": [
    "#  Importing all the necessary libraries for EDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 1.5.3\n",
      "numpy version: 1.24.3\n",
      "re version: 2.2.1\n",
      "seaborn version: 0.12.2\n",
      "matplotlib version: 3.7.1\n",
      "xgboost version: 1.7.6\n",
      "mrmr version: 0.2.8\n",
      "sklearn version: 1.3.1\n",
      "scipy version: 1.10.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import xgboost as xgb\n",
    "import mrmr\n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "libraries = {\n",
    "    'pandas': pd,\n",
    "    'numpy': np,\n",
    "    're': re,\n",
    "    'seaborn': sns,\n",
    "    'matplotlib': matplotlib,\n",
    "    'xgboost': xgb,\n",
    "    'mrmr': mrmr,\n",
    "    'sklearn': sklearn,\n",
    "    'scipy': scipy\n",
    "}\n",
    "\n",
    "for lib, module in libraries.items():\n",
    "    try:\n",
    "        print(f'{lib} version: {module.__version__}')\n",
    "    except AttributeError:\n",
    "        print(f'{lib} does not have a __version__ attribute')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8X4nRzYP7Hh",
    "outputId": "02c6f914-33bf-4c30-d957-b5582fc33849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\rishi\\anaconda3\\lib\\site-packages (1.7.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from xgboost) (1.10.1)\n",
      "Requirement already satisfied: mrmr_selection in c:\\users\\rishi\\anaconda3\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: category-encoders in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from mrmr_selection) (2.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from mrmr_selection) (3.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from mrmr_selection) (4.65.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from mrmr_selection) (1.2.0)\n",
      "Requirement already satisfied: pandas>=1.0.3 in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from mrmr_selection) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.18.1 in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from mrmr_selection) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from mrmr_selection) (1.3.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from mrmr_selection) (1.10.1)\n",
      "Requirement already satisfied: polars>=0.12.5 in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from mrmr_selection) (0.18.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from pandas>=1.0.3->mrmr_selection) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from pandas>=1.0.3->mrmr_selection) (2022.7)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from category-encoders->mrmr_selection) (0.14.0)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from category-encoders->mrmr_selection) (0.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from scikit-learn->mrmr_selection) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from jinja2->mrmr_selection) (2.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from tqdm->mrmr_selection) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from patsy>=0.5.1->category-encoders->mrmr_selection) (1.16.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\rishi\\anaconda3\\lib\\site-packages (from statsmodels>=0.9.0->category-encoders->mrmr_selection) (23.0)\n"
     ]
    }
   ],
   "source": [
    "# For mrmr algorithm\n",
    "!pip install xgboost\n",
    "!pip install mrmr_selection\n",
    "from mrmr import mrmr_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "twp99ASJP9Eb"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./CancerCell2022_AZD4547_PRISM.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-272GvtVQX6U"
   },
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "XbX-aXjNQLgU",
    "outputId": "cce3e8e5-651f-47a7-f0e6-e7fa42e3b081"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cell line</th>\n",
       "      <th>P37108</th>\n",
       "      <th>Q96JP5</th>\n",
       "      <th>Q9Y4H2</th>\n",
       "      <th>P36578</th>\n",
       "      <th>Q6SPF0</th>\n",
       "      <th>O76031</th>\n",
       "      <th>Q8WUQ7</th>\n",
       "      <th>A6NIH7</th>\n",
       "      <th>Q9BTD8</th>\n",
       "      <th>...</th>\n",
       "      <th>Q5EBL4</th>\n",
       "      <th>P49715</th>\n",
       "      <th>Q5TA45</th>\n",
       "      <th>O14924</th>\n",
       "      <th>Q7Z3B1</th>\n",
       "      <th>O60669</th>\n",
       "      <th>Q13571</th>\n",
       "      <th>Q96JM2</th>\n",
       "      <th>P35558</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACH_000007</td>\n",
       "      <td>70.813376</td>\n",
       "      <td>10.397105</td>\n",
       "      <td>7.838241</td>\n",
       "      <td>245.716342</td>\n",
       "      <td>6.361293</td>\n",
       "      <td>33.219996</td>\n",
       "      <td>0.072112</td>\n",
       "      <td>3.190716</td>\n",
       "      <td>10.932641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016581</td>\n",
       "      <td>0.047859</td>\n",
       "      <td>6.687097</td>\n",
       "      <td>0.196153</td>\n",
       "      <td>0.010548</td>\n",
       "      <td>0.036507</td>\n",
       "      <td>0.015176</td>\n",
       "      <td>0.006032</td>\n",
       "      <td>0.090045</td>\n",
       "      <td>1.750872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACH_000012</td>\n",
       "      <td>45.885932</td>\n",
       "      <td>0.219851</td>\n",
       "      <td>0.038735</td>\n",
       "      <td>95.065502</td>\n",
       "      <td>0.207778</td>\n",
       "      <td>43.659388</td>\n",
       "      <td>0.006217</td>\n",
       "      <td>0.020412</td>\n",
       "      <td>5.581528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007198</td>\n",
       "      <td>0.024734</td>\n",
       "      <td>0.156403</td>\n",
       "      <td>0.019394</td>\n",
       "      <td>0.146396</td>\n",
       "      <td>0.067847</td>\n",
       "      <td>0.011204</td>\n",
       "      <td>3.053069</td>\n",
       "      <td>0.005197</td>\n",
       "      <td>0.935781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACH_000015</td>\n",
       "      <td>97.156593</td>\n",
       "      <td>0.081679</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>223.416202</td>\n",
       "      <td>16.762669</td>\n",
       "      <td>18.645040</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>4.820324</td>\n",
       "      <td>9.788311</td>\n",
       "      <td>...</td>\n",
       "      <td>1.034375</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.084437</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.047362</td>\n",
       "      <td>0.534410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACH_000018</td>\n",
       "      <td>88.962782</td>\n",
       "      <td>16.162629</td>\n",
       "      <td>0.045823</td>\n",
       "      <td>167.838514</td>\n",
       "      <td>8.445893</td>\n",
       "      <td>18.903743</td>\n",
       "      <td>0.131190</td>\n",
       "      <td>0.194669</td>\n",
       "      <td>12.278054</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930270</td>\n",
       "      <td>0.023918</td>\n",
       "      <td>0.096784</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>6.919319</td>\n",
       "      <td>0.007905</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>0.061330</td>\n",
       "      <td>0.010841</td>\n",
       "      <td>1.034731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACH_000019</td>\n",
       "      <td>66.102366</td>\n",
       "      <td>0.597581</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>109.146344</td>\n",
       "      <td>31.741095</td>\n",
       "      <td>48.016222</td>\n",
       "      <td>0.011203</td>\n",
       "      <td>0.210224</td>\n",
       "      <td>9.051519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043006</td>\n",
       "      <td>0.008505</td>\n",
       "      <td>0.006465</td>\n",
       "      <td>0.131165</td>\n",
       "      <td>0.013341</td>\n",
       "      <td>0.319023</td>\n",
       "      <td>0.021714</td>\n",
       "      <td>0.319655</td>\n",
       "      <td>0.005729</td>\n",
       "      <td>1.125705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6694 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Cell line     P37108     Q96JP5    Q9Y4H2      P36578     Q6SPF0  \\\n",
       "0  ACH_000007  70.813376  10.397105  7.838241  245.716342   6.361293   \n",
       "1  ACH_000012  45.885932   0.219851  0.038735   95.065502   0.207778   \n",
       "2  ACH_000015  97.156593   0.081679  0.000352  223.416202  16.762669   \n",
       "3  ACH_000018  88.962782  16.162629  0.045823  167.838514   8.445893   \n",
       "4  ACH_000019  66.102366   0.597581  0.015659  109.146344  31.741095   \n",
       "\n",
       "      O76031    Q8WUQ7    A6NIH7     Q9BTD8  ...    Q5EBL4    P49715  \\\n",
       "0  33.219996  0.072112  3.190716  10.932641  ...  0.016581  0.047859   \n",
       "1  43.659388  0.006217  0.020412   5.581528  ...  0.007198  0.024734   \n",
       "2  18.645040  0.000067  4.820324   9.788311  ...  1.034375  0.001502   \n",
       "3  18.903743  0.131190  0.194669  12.278054  ...  7.930270  0.023918   \n",
       "4  48.016222  0.011203  0.210224   9.051519  ...  0.043006  0.008505   \n",
       "\n",
       "     Q5TA45    O14924    Q7Z3B1    O60669    Q13571    Q96JM2    P35558  \\\n",
       "0  6.687097  0.196153  0.010548  0.036507  0.015176  0.006032  0.090045   \n",
       "1  0.156403  0.019394  0.146396  0.067847  0.011204  3.053069  0.005197   \n",
       "2  0.084437  0.002895  0.003559  0.000073  0.000333  0.003592  0.047362   \n",
       "3  0.096784  0.000795  6.919319  0.007905  0.007362  0.061330  0.010841   \n",
       "4  0.006465  0.131165  0.013341  0.319023  0.021714  0.319655  0.005729   \n",
       "\n",
       "        AUC  \n",
       "0  1.750872  \n",
       "1  0.935781  \n",
       "2  0.534410  \n",
       "3  1.034731  \n",
       "4  1.125705  \n",
       "\n",
       "[5 rows x 6694 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num = df.rename(columns = {\"Row\" : \"Cell line\"})\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cjuCPBOBQd8O",
    "outputId": "100119e1-4b21-4cd4-a3e1-9689992f33b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 336 entries, 0 to 335\n",
      "Columns: 6694 entries, Cell line to AUC\n",
      "dtypes: float64(6693), object(1)\n",
      "memory usage: 17.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, 6694)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "GRShqBPEQf7W",
    "outputId": "9bcf6377-afc1-4589-f714-b4a5453d7892",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_num.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "vLaSm-Z1QhEk",
    "outputId": "228a929e-5a5a-4253-c78e-14632c78a418"
   },
   "outputs": [],
   "source": [
    "# For categorical variables\n",
    "df_num.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlyiAFSgmHCx"
   },
   "source": [
    "## 1. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCQpg71vQjH8",
    "outputId": "70133c14-1639-4085-f498-55bb2617a26f"
   },
   "outputs": [],
   "source": [
    "# Checking for columns with NaN values (Missing Values)\n",
    "[features for features in df_num.columns if df_num[features].isna().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRLgoh-US6Hq"
   },
   "source": [
    "**AUC** is the target columns and has null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QkFxa4D6B8Z",
    "outputId": "d720c42a-5f85-473c-9c3f-ceb08ed0ba5a"
   },
   "outputs": [],
   "source": [
    "# Checking the number of missing values in AUC\n",
    "df_num['AUC'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVB_3UoETWF1"
   },
   "outputs": [],
   "source": [
    "# Deleting rows with NaN\n",
    "df_num = df_num.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIuclgU_TAqr",
    "outputId": "b64f2359-8cfd-4c64-bf25-2926925e1f2e"
   },
   "outputs": [],
   "source": [
    "df_num.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VeKHBf8ZT5LO",
    "outputId": "1873418c-f111-41a4-c60c-792642680a4b"
   },
   "outputs": [],
   "source": [
    "# Checking for duplicate rows in the dataset\n",
    "df_num.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nwrwbx63T1wZ",
    "outputId": "c73b765f-08bc-49eb-e05a-3721796b1280"
   },
   "outputs": [],
   "source": [
    "df_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94HElq-MylDo"
   },
   "outputs": [],
   "source": [
    "df_num.drop([\"Cell line\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "c9VLhiLryVBl",
    "outputId": "5c378e20-3e76-4422-9ec0-662c168e60b0"
   },
   "outputs": [],
   "source": [
    "df_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cITMhhAOYYQi"
   },
   "source": [
    "## 2. Bootstrapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Atke5UpGYN-H"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_data(dataset):\n",
    "    print(\"Bootstrapping dataset\")\n",
    "    return resample(dataset, replace = True, n_samples = len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up K Fold to be used later in Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7gaJyDfJZsr"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Defining a function to normalize the dataset\n",
    "\n",
    "def norm(data):\n",
    "    \"\"\"\n",
    "    The function takes a pandas dataset as input and returns a normalized pandas dataset (using Standard Scaler) as output\n",
    "    \"\"\"\n",
    "    print(\">Normalizing dataset\")\n",
    "    # Initializing Standard Scaler for normalizing dataset\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    \n",
    "    return pd.DataFrame(normalized_data, columns = data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpwOe7M4Q8lV"
   },
   "source": [
    "## 5. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0dusB3UQ_Af"
   },
   "source": [
    "## Filter methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-iN2O5cRDV_"
   },
   "source": [
    "### 1. Pearson's Correlation (Linear Correlation)\n",
    "\n",
    "Our objective is to find all features with p_value <0.05 and high correlation (negative and positive) with respect to the target value,  **AUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_pearson(X, y):\n",
    "    \"\"\"\n",
    "    This function takes the dataframe and the target variable as the input arguments and forms a dictionary with the features as keys\n",
    "    and the corresponding correlation and p values as values. It then converts the dictionary into a pandas dataframe and returns only \n",
    "    those features with p_value < 0.05 as function output.\n",
    "    \"\"\"\n",
    "    print(\">Feature selection using Pearson correlation\")\n",
    "    \n",
    "    corr_n_p = {}\n",
    "\n",
    "    for column in X:\n",
    "        # pearsonr returns a tuple (Pearson's correlation coefficient, 2-tailed p-value)\n",
    "        corr, p_value = pearsonr(X[column], y)\n",
    "        corr_n_p[column] = (corr, p_value)\n",
    "\n",
    "    correlation_df = pd.DataFrame.from_dict(corr_n_p, orient='index', columns=['correlation', 'p_value'])\n",
    "    \n",
    "    # Reset the index to move the current index (genes) as a proper column\n",
    "    correlation_df.reset_index(inplace=True)\n",
    "    # Rename the column to 'genes'\n",
    "    correlation_df.rename(columns={'index': 'genes'}, inplace=True)\n",
    "    \n",
    "    # Plotting the graph of correlation vs p-value\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.scatterplot(data=correlation_df, x='correlation', y='p_value', alpha=0.6, edgecolor=None)\n",
    "    plt.axhline(y=0.05, color='r', linestyle='--')\n",
    "    plt.title('Correlation vs P-Value')\n",
    "    plt.xlabel('Correlation')\n",
    "    plt.ylabel('P-Value')\n",
    "    plt.show()\n",
    "    \n",
    "    # Filtering and sorting the significant features\n",
    "    significant_df = correlation_df[correlation_df.p_value < 0.05].sort_values(by='p_value', ascending=True)\n",
    "    \n",
    "    significant_df.reset_index(inplace=True)\n",
    "    significant_df.drop(columns=\"index\", inplace=True)\n",
    "    \n",
    "    # Filtering out the significant features from the actual dataset\n",
    "    features = list(significant_df.genes)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhPY36U8aGli"
   },
   "source": [
    "### 3. MRMR (Maximum Relevance and Minimum Redundancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to implement mRMR feature selection\n",
    "def mrmr_feature_selection(X,y,k):\n",
    "    # the different column types in the dataset\n",
    "    print(\">Implementing MRMR feature selection\")\n",
    "    return mrmr_regression(X =X, y= y, K = k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0IsqNUvcbhp"
   },
   "source": [
    "### 4. F-regression (F-statistic and p-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJH1xTB-an62"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to implement F-regression\n",
    "def f_reg_feature_selection(X, y, k):\n",
    "    print(\">Implementing F-regression for feature selection\")\n",
    "    # Using SelectKBest to select the best features\n",
    "    selector = SelectKBest(f_regression, k = k)\n",
    "    new = selector.fit_transform(X,y)\n",
    "    \n",
    "    f_selected = X.columns[selector.get_support()].to_list()\n",
    "    \n",
    "    return f_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "jCTVZIflc7Cc",
    "outputId": "3f568164-0cb0-47ee-bef3-c4d5b0142009"
   },
   "source": [
    "## Embedded Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLO-WFRzc7X9"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_cv(X,y):\n",
    "    print(\">Implementing Lasso regularization for feature selection\")\n",
    "    \n",
    "    # Initializing an array of different values for alpha\n",
    "    alphas = np.logspace(-4,4,50)\n",
    "    \n",
    "    # Ensure X and y have matching indices\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    \n",
    "    # Use LassoCV to find the best alpha using CV\n",
    "    lassocv = LassoCV(alphas = alphas, cv = 5)\n",
    "    lassocv.fit(X,y)\n",
    "    \n",
    "    best_alpha = lassocv.alpha_\n",
    "    print(f\"    best alpha value: {best_alpha}\")\n",
    "    \n",
    "    # Using the best alpha to implemene Lasso Regularization\n",
    "    lasso = Lasso(alpha=best_alpha)\n",
    "\n",
    "    # Fit the Lasso model\n",
    "    lasso.fit(X, y)\n",
    "\n",
    "    # Identify features with non-zero coefficients\n",
    "    lasso_features = np.where(lasso.coef_ != 0)[0]\n",
    "    \n",
    "    return lasso_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "def xgb_regressor(X,y):\n",
    "    \n",
    "    xgb_results = {}\n",
    "    print('Fitting XGBoost Model')\n",
    "    # Create a XGBoost Regressor object\n",
    "    xgbr = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=0, n_jobs=-1)\n",
    "    \n",
    "    # Param grid for XGBoost\n",
    "    param_grid_xgb = {\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "    'reg_lambda': [0.2, 0.5, 0.8, 1, 1.2],\n",
    "    'reg_alpha': [0, 0.2, 0.5, 0.8, 1],\n",
    "    'n_estimators': [100, 150, 200, 250, 300],\n",
    "    'min_child_weight': [1, 2, 3, 4],\n",
    "    'max_depth': [5, 6, 7, 8, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9]}\n",
    "\n",
    "\n",
    "    model = xgb.XGBRegressor()\n",
    "\n",
    "    kf = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "\n",
    "    grid_search = RandomizedSearchCV(model, param_grid_xgb, cv=kf, verbose=1)\n",
    "    \n",
    "    grid_search.fit(X,y)\n",
    "    \n",
    "    print(f\"XGBoost Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"XGBoost Best Score: {grid_search.best_score_}\")\n",
    "    \n",
    "    xgb_results['best_params'] = grid_search.best_params_\n",
    "    xgb_results['best_score'] = grid_search.best_score_\n",
    "    \n",
    "    return xgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "def svmachine(X,y):\n",
    "    \n",
    "    svm_results = {}\n",
    "    \n",
    "    print('Fitting Support Vector Machine Model')\n",
    "    \n",
    "    # Param grid for SVM\n",
    "    param_grid_svm = {\n",
    "    'shrinking': [True, False],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid', 'linear'],\n",
    "    'gamma': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5],\n",
    "    'epsilon': [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "    'degree': [2, 3, 4, 5, 6],\n",
    "    'coef0': [-1, -0.5, 0, 0.5, 1, 1.5],\n",
    "    'C': [0.1, 0.5, 1, 5, 10, 25, 50, 100]}\n",
    "    \n",
    "    model = SVR()\n",
    "\n",
    "    kf = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "\n",
    "    grid_search = RandomizedSearchCV(model, param_grid_svm, cv=kf, verbose=1, n_jobs=-1)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    print(f\"SVM Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"SVM Best Score: {grid_search.best_score_}\")\n",
    "    \n",
    "    svm_results['best_params'] = grid_search.best_params_\n",
    "    svm_results['best_score'] = grid_search.best_score_\n",
    "    \n",
    "    return svm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP (Neural Network)\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def mlp_regressor(X,y):\n",
    "    \n",
    "    mlp_results = {}\n",
    "    \n",
    "    # Param Grid for MLP\n",
    "    param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50, 50), (100, 100), (50, 50, 50), (150, 150), (100, 100, 100), (50, 50, 50, 50)],\n",
    "    'activation': ['logistic', 'tanh', 'relu', 'identity'],\n",
    "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1],\n",
    "    'learning_rate': ['constant', 'adaptive', 'invscaling'],\n",
    "    'learning_rate_init': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05],\n",
    "    'max_iter': [50, 100, 200, 300, 400, 500],\n",
    "    'momentum': [0.1, 0.5, 0.7, 0.9, 0.95],\n",
    "    'beta_1': [0.7, 0.8, 0.9, 0.95],\n",
    "    'beta_2': [0.99, 0.995, 0.999]}\n",
    "\n",
    "    \n",
    "    print('Fitting Multi-Layer Perceptron Model (Neural Network)')\n",
    "    \n",
    "    kf = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize MLPRegressor and GridSearchCV\n",
    "    mlp = MLPRegressor(max_iter=1000) # You might want to increase max_iter if the model doesn't converge\n",
    "    grid_search = RandomizedSearchCV(mlp, param_grid_mlp, n_jobs=-1, cv=kf, verbose=1)\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    print(f\"MLP Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"MLP Best Score: {grid_search.best_score_}\")\n",
    "    \n",
    "    mlp_results['best_params'] = grid_search.best_params_\n",
    "    mlp_results['best_score'] = grid_search.best_score_\n",
    "    \n",
    "    return mlp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to fit models\n",
    "def model_fitting(X, y):\n",
    "    print(\">Fitting models\")\n",
    "    \n",
    "    results = {}\n",
    "    scores = []\n",
    "    model_names = []\n",
    "    \n",
    "    # Cross validation object\n",
    "    kf = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "    print('-- Fitting models on default parameters...')\n",
    "    models = {\n",
    "        'SVM': SVR(),\n",
    "        'MLP': MLPRegressor(),\n",
    "        'XGBoost': xgb.XGBRegressor()\n",
    "    }\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        \n",
    "        cv_score_list = cross_val_score(model, X, y, cv = kf)\n",
    "        results[f'{name}'] = (np.mean(cv_score_list)) \n",
    "    \n",
    "    # Selecting the models with the best scores on default values\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_2_keys = [sorted_results[i][0] for i in range(2)]\n",
    "    \n",
    "    tuning_models = {}\n",
    "    \n",
    "    print(f'--Top 2 models on default parameters:{top_2_keys}')\n",
    "    print('-- Hyperparameter tuning the models')\n",
    "    for mod in  top_2_keys:\n",
    "        if mod == 'SVM':\n",
    "            svm_results = svmachine(X, y)\n",
    "            tuning_models['SVM'] = svm_results\n",
    "        elif mod == 'MLP':\n",
    "            mlp_results = mlp_regressor(X, y)\n",
    "            tuning_models['MLP'] = mlp_results\n",
    "        elif mod == 'XGBoost':\n",
    "            xgb_results = xgb_regressor(X, y)\n",
    "            tuning_models['XGBoost'] = xgb_results\n",
    "    \n",
    "    tuning_results = {}\n",
    "    scores = []\n",
    "    model_names = []\n",
    "    \n",
    "    for name, model in tuning_models.items():\n",
    "        \n",
    "        tuning_results[f'{name}_score'] = (model['best_score'], model['best_params'])\n",
    "        # for plotting\n",
    "        scores.append(model['best_score'])\n",
    "        model_names.append(name)\n",
    "            \n",
    "    # Plotting the scores\n",
    "    plt.figure(figsize=(3, 2))\n",
    "    plt.barh(model_names, scores, color=['blue', 'red', 'green', 'yellow'])\n",
    "    plt.xlabel('Best Score')\n",
    "    plt.title('Model Performance')\n",
    "    plt.gca().invert_yaxis()  # To display the model with the lowest MSE at the top\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"....Model Fitting Done\")\n",
    "    top2 = sorted(tuning_results.items(), key=lambda x: x[1][0], reverse=True)[:2]\n",
    "    print(top2)\n",
    "    return top2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Feature Selection and Backward Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_selection(X, y, top2_models):\n",
    "    \"\"\"\n",
    "    This function performs forward feature selection and backward feature elimination on the dataset\n",
    "    for the top 2 models and returns the selected features for each model and method.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initializing dictionary to store results\n",
    "    final_features = {}\n",
    "    results = {}\n",
    "    forward_counts = []\n",
    "    backward_counts = []\n",
    "    model_names = []\n",
    "    \n",
    "    \n",
    "    # Extracting the best estimator from the models' results\n",
    "    best_model = top2_models[0][0].split('_')[0]\n",
    "    print(best_model)\n",
    "    \n",
    "    for model_name, (score, params) in top2_models[:1]:\n",
    "        if 'MLP' in model_name.upper():\n",
    "            model = MLPRegressor(**params)\n",
    "        \n",
    "        elif 'SVM' in model_name.upper():\n",
    "            model = SVR(**params)\n",
    "\n",
    "        elif 'XGBOOST' in model_name.upper():\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f'Unknown Model name: {model_name}')\n",
    "\n",
    "        # Forward Feature Selection\n",
    "        print(f\">Running Forward Feature Selection for {model_name}\")\n",
    "\n",
    "        # Forward Selection\n",
    "        sfs_forward = SequentialFeatureSelector(model, direction='forward', cv=kf, n_features_to_select=20, n_jobs=-1)\n",
    "        sfs_forward.fit(X, y)\n",
    "        forward_features = X.columns[sfs_forward.get_support()]\n",
    "        print(f'Forward features:{forward_features}')\n",
    "        \n",
    "        print(f\">Running Backward Feature Selection for {model_name}\")\n",
    "        # Backward Selection\n",
    "        sfs_backward = SequentialFeatureSelector(model, direction='backward', cv=kf, n_features_to_select=20, n_jobs=-1)\n",
    "        sfs_backward.fit(X, y)\n",
    "        backward_features = X.columns[sfs_backward.get_support()]\n",
    "        print(f'Backward features:{backward_features}')\n",
    "\n",
    "\n",
    "        # Store results\n",
    "        results[model_name.split('_')[0] + \"_forward\"] = forward_features,\n",
    "        results[model_name.split('_')[0] + \"_backward\"] = backward_features\n",
    "    \n",
    "    print(results)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(dataset, n_bootstraps):\n",
    "    kf = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "    \n",
    "    # DataFrame to store the results for each bootstrap iteration\n",
    "    results_df = pd.DataFrame(columns=['Bootstrap Iteration Count', 'Best Model',\n",
    "                                       'Best Model Forward Features', 'Best Model Backward Features','Best Model Score'])\n",
    "    \n",
    "    # Iterator for bootstraps\n",
    "    count = 1\n",
    "    \n",
    "    # Create the csv file with headers if it doesn't exist\n",
    "    csv_file = 'bootstrap_final.csv'\n",
    "    \n",
    "    if not os.path.exists(csv_file):\n",
    "        results_df.to_csv(csv_file, index = False)\n",
    "    \n",
    "    for _ in range(n_bootstraps):\n",
    "        print(\"Bootstrapped sample {} of {}\".format(count, n_bootstraps))\n",
    "        \n",
    "        # Bootstrapping\n",
    "        bootstrapped_data = bootstrap_data(dataset)\n",
    "        \n",
    "        # Train and test\n",
    "        X = bootstrapped_data.drop(columns=['AUC'])\n",
    "        y = bootstrapped_data['AUC']\n",
    "        X = X.reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "        # Normalizing\n",
    "        X_normalized = norm(X)\n",
    "        \n",
    "        # Feature Selection\n",
    "        # Pearson\n",
    "        pearson_features = corr_pearson(X_normalized, y)\n",
    "        \n",
    "        if len(pearson_features)>500:\n",
    "            # MRMR\n",
    "            mrmr_features = mrmr_feature_selection(X_normalized[pearson_features], y, 500)\n",
    "            # F-regression\n",
    "            f_features = f_reg_feature_selection(X_normalized[pearson_features], y, 500)\n",
    "            # Common between mrmr and f-regression\n",
    "            common_features = list(set(mrmr_features) & set(f_features))\n",
    "            print(\"    Number of common features between mrmr and f-regression:{}\".format(len(common_features)))\n",
    "            \n",
    "        else:\n",
    "            common_features = pearson_features\n",
    "        \n",
    "        common_features_df = pd.DataFrame(common_features)\n",
    "        # Write the DataFrame to a CSV file\n",
    "        common_features_df.to_csv('./filter_fs_df.csv', index=False)\n",
    "        # Lasso\n",
    "        embedded_features_indices = lasso_cv(X_normalized[common_features], y)\n",
    "        embedded_features = X_normalized.columns[embedded_features_indices].to_list()\n",
    "        \n",
    "        embedded_df = pd.DataFrame(embedded_features)\n",
    "        # Write the DataFrame to a CSV file\n",
    "        embedded_df.to_csv('./embedded_df.csv', index=False)\n",
    "        \n",
    "        print(\"    Number of common features left after Lasso:{}\".format(len(embedded_features)))\n",
    "        \n",
    "        # Model Selection\n",
    "        top2_models = model_fitting(X_normalized[embedded_features], y)\n",
    "        \n",
    "        \n",
    "        # Extracting model names\n",
    "        best_model_name = top2_models[0][0].split('_')[0]  # Extracting the model name from the result tuple\n",
    "        best_model_r2 = top2_models[0][1][0]\n",
    "        print(f\"    The best model is {best_model_name} with R2 score of {best_model_r2}\")\n",
    "        \n",
    "        if len(embedded_features)>20: # We are trying to filter out the best 20 features\n",
    "            # FFS, BFE\n",
    "            final_features = forward_backward_selection(X_normalized[embedded_features], y, top2_models)\n",
    "\n",
    "            # Extracting forward and backward features for best and second best models\n",
    "            best_model_forward_features = final_features.get(best_model_name + \"_forward\", [])\n",
    "            best_model_backward_features = final_features.get(best_model_name + \"_backward\", [])\n",
    "            \n",
    "        else:\n",
    "            best_model_forward_features = embedded_features\n",
    "            best_model_backward_features = embedded_features\n",
    "        \n",
    "        \n",
    "        # Appending the results to the results dataframe\n",
    "        new_result = {\n",
    "            'Best Model': best_model_name,\n",
    "            'Best Model Forward Features': best_model_forward_features,\n",
    "            'Best Model Backward Features': best_model_backward_features,\n",
    "            'Best Model Score': best_model_r2}\n",
    "        \n",
    "        results_df = results_df.append(new_result, ignore_index=True)\n",
    "        \n",
    "        # Appending to df in the .csv file\n",
    "        pd.DataFrame([new_result]).to_csv(csv_file, mode = 'a', header=False, index=False)\n",
    "        \n",
    "        print(results_df)\n",
    "        print('Data appended to', csv_file)\n",
    "        print(\"----------------------------------------------------------\\n\")\n",
    "        count+=1\n",
    "        \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pipeline(df_num, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
